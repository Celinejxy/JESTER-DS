{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ac2fbc44",
   "metadata": {},
   "source": [
    "### JESTER JOKE RECOMMENDER SYSTEM \n",
    "##### &emsp;&emsp;- Collaborative Filtering -Item-Based Recommender Algorithm \n",
    "##### _Over 1.7 million ratings of 150 jokes from 59,132 users_\n",
    "Xuyang Ji <br> Mar 9st, 2023"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "921de7a0",
   "metadata": {},
   "source": [
    ">_Data Abstract: The dataset \"jester_ratings.csv\", collected between Nov 2006 - May 2009, contains over 1.7 million continuous ratings (-10.00 to +10.00) of 150 jokes from 59,132 users, where each row is formatted as userID, itemID, and ratings. The dataset \"jester_item.csv\" maps item ID to the actual text of the jokes._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a1aaf756",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/celine/Desktop/JESTER DS'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "07a8ecc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "from numpy import *\n",
    "from numpy import linalg as la\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from collections import Counter #finding the majority \n",
    "from sklearn.pipeline import Pipeline,make_pipeline\n",
    "from sklearn.decomposition import TruncatedSVD,PCA #Dimension Reduction (LSA)\n",
    "from sklearn.preprocessing import Normalizer, LabelBinarizer \n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a0439bf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "rating_df= pd.read_csv(\"jester_ratings.csv\")\n",
    "items_df= pd.read_csv(\"jester_items.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "29c6c819",
   "metadata": {},
   "outputs": [],
   "source": [
    "rating_df= rating_df.pivot(index='userId',columns='jokeId', values='rating')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "59ba4b97",
   "metadata": {},
   "outputs": [],
   "source": [
    "scalar= MinMaxScaler(feature_range=(1,21))\n",
    "rating_df[rating_df.columns]= scalar.fit_transform(rating_df[rating_df.columns])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5943ab34",
   "metadata": {},
   "outputs": [],
   "source": [
    "rating_df.fillna(0,inplace=True)\n",
    "rating_df= rating_df.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1d1b6dc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#inA and inB are coluumn vectors \n",
    "def ecludSim(inA,inB):\n",
    "    return 1.0 / (1.0 + la.norm(inA - inB))\n",
    "\n",
    "def pearsSim(inA,inB):\n",
    "    if len(inA) < 3 : \n",
    "        return 1.0\n",
    "    return 0.5 + 0.5 * np.corrcoef(inA, inB, rowvar = 0)[0][1]\n",
    "\n",
    "def cosSim(inA,inB):\n",
    "    num = float(inA.T * inB)\n",
    "    denom = la.norm(inA)*la.norm(inB)\n",
    "    return 0.5 + 0.5 * (num / denom)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8892d9c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5381343806127434"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosSim(mat(rating_df)[:,0], mat(rating_df)[:,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9224fe71",
   "metadata": {},
   "outputs": [],
   "source": [
    "#item-based recommender sys.\n",
    "def standEst(dataMat, user, simMeas, item):\n",
    "    #calculates the estimated rating a user would give an item for \n",
    "    #a given similarity measure. \n",
    "    n = shape(dataMat)[1]\n",
    "    simTotal = 0.0; ratSimTotal = 0.0\n",
    "    for j in range(n):\n",
    "        userRating = dataMat[user,j]\n",
    "        if userRating == 0: \n",
    "            continue\n",
    "        # Find items rated by users and find indices of the trues in the array\n",
    "        overLap = np.nonzero(np.array(np.logical_and(dataMat[:,item]>0, \\\n",
    "                                      dataMat[:,j]>0)))[0]\n",
    "        if len(overLap) == 0: \n",
    "            similarity = 0\n",
    "        else: \n",
    "            similarity = simMeas(dataMat[overLap,item], \\\n",
    "                                   dataMat[overLap,j])\n",
    "        #print 'the %d and %d similarity is: %f' % (item, j, similarity)\n",
    "        simTotal += similarity\n",
    "        ratSimTotal += similarity * userRating\n",
    "    if simTotal == 0: \n",
    "        return 0\n",
    "    else: \n",
    "        return ratSimTotal/simTotal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "eb6dd124",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10.891311834574973"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "standEst(rating_df,5,ecludSim,6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9989fbbf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "U,Sigma,VT = la.svd(mat(rating_df), full_matrices=False)\n",
    "sum((Sigma**2)[:50]) > sum(Sigma**2)*0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6f930ed1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#using SVD \n",
    "def svdEst(data, user, simMeas, item):\n",
    "    #Estimate rating for a given items for a given user by using SVD\n",
    "    n = shape(data)[1]\n",
    "    simTotal = 0.0; ratSimTotal = 0.0\n",
    "    data=mat(data)\n",
    "    U,Sigma,VT = la.svd(data, full_matrices=False)\n",
    "    Sig4 = np.mat(np.eye(50)*Sigma[:50]) #arrange Sig4 into a diagonal matrix\n",
    "    xformedItems = data.T * U[:,:50] * Sig4.I  #create transformed items\n",
    "    for j in range(n):\n",
    "        userRating = data[user,j]\n",
    "        if userRating == 0 or j==item: continue\n",
    "        similarity = simMeas(xformedItems[item,:].T,\\\n",
    "                             xformedItems[j,:].T)\n",
    "        #print 'the %d and %d similarity is: %f' % (item, j, similarity)\n",
    "        simTotal += similarity\n",
    "        ratSimTotal += similarity * userRating\n",
    "    if simTotal == 0: return 0\n",
    "    else: return ratSimTotal/simTotal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "57b734f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8.64383732106096"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svdEst(rating_df,5,ecludSim,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "de5bd162",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validate_user(dataMat, user, test_ratio, estMethod=standEst, simMeas=pearsSim):\n",
    "    number_of_items = np.shape(dataMat)[1]\n",
    "    \n",
    "    rated_items_by_user = np.array([i for i in range(number_of_items) if dataMat[user,i]>0])\n",
    "    test_size = int(test_ratio * len(rated_items_by_user))\n",
    "    test_indices = np.random.randint(0, len(rated_items_by_user), test_size)\n",
    "    \n",
    "    withheld_items = rated_items_by_user[test_indices]\n",
    "    original_user_profile = np.copy(dataMat[user])\n",
    "    dataMat[user, withheld_items] = 0 # So that the withheld test items is not used in the rating estimation below\n",
    "    error_u = 0.0\n",
    "    count_u = len(withheld_items)\n",
    "\n",
    "\t# Compute absolute error for user u over all test items\n",
    "    for item in withheld_items:\n",
    "        # Estimate rating on the withheld item\n",
    "        estimatedScore = estMethod(dataMat, user, simMeas, item)\n",
    "        error_u = error_u + (estimatedScore - original_user_profile[item])**2\n",
    "    \n",
    "    for item in withheld_items:\n",
    "        dataMat[user, item] = original_user_profile[item]\n",
    "    return error_u, count_u\n",
    "\n",
    "\n",
    "def test(dataMat, test_ratio, estMethod):\n",
    "    # iterate over all users and for each \n",
    "    # perform evaluation by calling the above cross_validate_user function on each user.\n",
    "    total_error=0.0\n",
    "    total_count=0\n",
    "    for user in range(dataMat.shape[0]):\n",
    "        if estMethod == standEst:\n",
    "            error_u, count_u = cross_validate_user(dataMat,user,test_ratio, standEst)\n",
    "            total_error += error_u\n",
    "            total_count += count_u\n",
    "        if estMethod == svdEst:\n",
    "            if user % 50 ==0:\n",
    "                error_u, count_u = cross_validate_user(dataMat,user,test_ratio,svdEst)\n",
    "                total_error += error_u\n",
    "                total_count += count_u\n",
    "    RMSE= np.sqrt(total_error/total_count)\n",
    "    print('Mean Absoloute Error for ',estMethod,' : ', RMSE)\n",
    "    return RMSE"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "412b62fe",
   "metadata": {},
   "source": [
    "By iterating all users and for each performs evaluation with 20% test ratio, the overall Mean Absolute Error (MAE) using standard item-based collaborative filtering based on the rating prediction function \"standEst\" is approximately 4.6. Meantime, the MAE of using the SVD-based version of the rating item-based CF as the prediction engine, is approximately 4. Notice that the computing time for using 'svdEst' function is much more efficient than using 'standEst' function/ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8a93d95f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absoloute Error for  <function standEst at 0x7fe572fea040>  :  4.581238825488467\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "4.581238825488467"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test(rating_df[::50], 0.2, standEst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "73cf6b99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absoloute Error for  <function svdEst at 0x7fe572fea0d0>  :  4.278577840402492\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "4.278577840402492"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test(rating_df[::50],0.2,svdEst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fbf3371f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_most_similar_jokes(dataMat, jokes, queryJoke, k, metric=pearsSim):\n",
    "    # Get the ratings for the query joke\n",
    "    queryJokeRatings = dataMat[:, queryJoke].flatten()\n",
    "    \n",
    "    # Calculate similarity between the query joke and all other jokes\n",
    "    similarities = []\n",
    "    for i in range(dataMat.shape[1]):\n",
    "        jokeRatings = dataMat[:, i].flatten()\n",
    "        similarity = metric(queryJokeRatings, jokeRatings)\n",
    "        similarities.append(similarity)\n",
    "    \n",
    "    # Sort the similarities in descending order and get the indices of the top-k similar jokes\n",
    "    sorted_similarities_indices = np.argsort(similarities)[::-1][:k]\n",
    "    \n",
    "    # Create a dataframe to store the top-k similar jokes and their similarity scores\n",
    "    similar_jokes_df = pd.DataFrame({'joke_id': sorted_similarities_indices, \n",
    "                                      'similarity_score': [similarities[i] for i in sorted_similarities_indices],\n",
    "                                      'joke_text': [jokes.loc[jokes['jokeId'] == i, 'jokeText'].iloc[0] for i in sorted_similarities_indices]})\n",
    "    \n",
    "    # Print the dataframe\n",
    "    print(f\"The {k} most similar jokes to joke {queryJoke} ({jokes.loc[jokes['jokeId'] == queryJoke, 'jokeText'].iloc[0]}) are:\")\n",
    "    print(similar_jokes_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "73affa31",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 5 most similar jokes to joke 3 (Q. What's 200 feet long and has 4 teeth? \n",
      "\n",
      "A. The front row at a Willie Nelson Concert.\n",
      ") are:\n",
      "   joke_id  similarity_score  \\\n",
      "0        3          1.000000   \n",
      "1        4          0.706858   \n",
      "2        2          0.695585   \n",
      "3        5          0.657029   \n",
      "4        6          0.643112   \n",
      "\n",
      "                                           joke_text  \n",
      "0  Q. What's 200 feet long and has 4 teeth? \\n\\nA...  \n",
      "1  Q. What's the difference between a man and a t...  \n",
      "2  This couple had an excellent relationship goin...  \n",
      "3  Q.\\tWhat's O. J. Simpson's Internet address? \\...  \n",
      "4  Bill & Hillary are on a trip back to Arkansas....  \n"
     ]
    }
   ],
   "source": [
    "print_most_similar_jokes(rating_df, items_df, 3, 5, pearsSim)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
