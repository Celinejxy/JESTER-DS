{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ac2fbc44",
   "metadata": {},
   "source": [
    "### JESTER JOKE RECOMMENDER SYSTEM \n",
    "##### &emsp;&emsp;- Collaborative Filtering -Based Recommender Algorithm \n",
    "##### _Over 1.7 million ratings of 150 jokes from 59,132 users_\n",
    "Xuyang Ji <br> Mar 9st, 2023"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "921de7a0",
   "metadata": {},
   "source": [
    ">_Data Abstract: The dataset \"jester_ratings.csv\", collected between Nov 2006 - May 2009, contains over 1.7 million continuous ratings (-10.00 to +10.00) of 150 jokes from 59,132 users, where each row is formatted as userID, itemID, and ratings. The dataset \"jester_item.csv\" maps item ID to the actual text of the jokes._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a1aaf756",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/celine/Desktop/JESTER DS'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "07a8ecc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "from numpy import *\n",
    "from numpy import linalg as la\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from collections import Counter #finding the majority \n",
    "from sklearn.pipeline import Pipeline,make_pipeline\n",
    "from sklearn.decomposition import TruncatedSVD,PCA #Dimension Reduction (LSA)\n",
    "from sklearn.preprocessing import Normalizer, LabelBinarizer \n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a0439bf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "rating_df= pd.read_csv(\"jester_ratings.csv\")\n",
    "items_df= pd.read_csv(\"jester_items.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "29c6c819",
   "metadata": {},
   "outputs": [],
   "source": [
    "rating_df= rating_df.pivot(index='userId',columns='jokeId', values='rating')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "59ba4b97",
   "metadata": {},
   "outputs": [],
   "source": [
    "scalar= MinMaxScaler(feature_range=(1,21))\n",
    "rating_df[rating_df.columns]= scalar.fit_transform(rating_df[rating_df.columns])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5943ab34",
   "metadata": {},
   "outputs": [],
   "source": [
    "rating_df.fillna(0,inplace=True)\n",
    "rating_df= rating_df.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1d1b6dc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#inA and inB are coluumn vectors \n",
    "def ecludSim(inA,inB):\n",
    "    return 1.0 / (1.0 + la.norm(inA - inB))\n",
    "\n",
    "def pearsSim(inA,inB):\n",
    "    if len(inA) < 3 : \n",
    "        return 1.0\n",
    "    return 0.5 + 0.5 * np.corrcoef(inA, inB, rowvar = 0)[0][1]\n",
    "\n",
    "def cosSim(inA,inB):\n",
    "    num = float(inA.T * inB)\n",
    "    denom = la.norm(inA)*la.norm(inB)\n",
    "    return 0.5 + 0.5 * (num / denom)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8892d9c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5381343806127434"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosSim(mat(rating_df)[:,0], mat(rating_df)[:,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9224fe71",
   "metadata": {},
   "outputs": [],
   "source": [
    "#item-based recommender sys.\n",
    "def standEst(dataMat, user, simMeas, item):\n",
    "    #calculates the estimated rating a user would give an item for \n",
    "    #a given similarity measure. \n",
    "    n = shape(dataMat)[1]\n",
    "    simTotal = 0.0; ratSimTotal = 0.0\n",
    "    for j in range(n):\n",
    "        userRating = dataMat[user,j]\n",
    "        if userRating == 0: \n",
    "            continue\n",
    "        # Find items rated by users and find indices of the trues in the array\n",
    "        overLap = np.nonzero(np.array(np.logical_and(dataMat[:,item]>0, \\\n",
    "                                      dataMat[:,j]>0)))[0]\n",
    "        if len(overLap) == 0: \n",
    "            similarity = 0\n",
    "        else: \n",
    "            similarity = simMeas(dataMat[overLap,item], \\\n",
    "                                   dataMat[overLap,j])\n",
    "        #print 'the %d and %d similarity is: %f' % (item, j, similarity)\n",
    "        simTotal += similarity\n",
    "        ratSimTotal += similarity * userRating\n",
    "    if simTotal == 0: \n",
    "        return 0\n",
    "    else: \n",
    "        return ratSimTotal/simTotal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "eb6dd124",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10.891311834574973"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "standEst(rating_df,5,ecludSim,6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9989fbbf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "U,Sigma,VT = la.svd(mat(rating_df), full_matrices=False)\n",
    "sum((Sigma**2)[:50]) > sum(Sigma**2)*0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "6f930ed1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#using SVD \n",
    "def svdEst(data, user, simMeas, item):\n",
    "    #Estimate rating for a given items for a given user by using SVD\n",
    "    n = shape(data)[1]\n",
    "    simTotal = 0.0; ratSimTotal = 0.0\n",
    "    data=mat(data)\n",
    "    U,Sigma,VT = la.svd(data, full_matrices=False)\n",
    "    Sig4 = np.mat(np.eye(50)*Sigma[:50]) #arrange Sig4 into a diagonal matrix\n",
    "    xformedItems = data.T * U[:,:50] * Sig4.I  #create transformed items\n",
    "    for j in range(n):\n",
    "        userRating = data[user,j]\n",
    "        if userRating == 0 or j==item: continue\n",
    "        similarity = simMeas(xformedItems[item,:].T,\\\n",
    "                             xformedItems[j,:].T)\n",
    "        #print 'the %d and %d similarity is: %f' % (item, j, similarity)\n",
    "        simTotal += similarity\n",
    "        ratSimTotal += similarity * userRating\n",
    "    if simTotal == 0: return 0\n",
    "    else: return ratSimTotal/simTotal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "57b734f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8.603118987836545"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svdEst(rating_df,5,ecludSim,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "de5bd162",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validate_user(dataMat, user, test_ratio, estMethod=standEst, simMeas=pearsSim):\n",
    "    number_of_items = np.shape(dataMat)[1]\n",
    "    \n",
    "    rated_items_by_user = np.array([i for i in range(number_of_items) if dataMat[user,i]>0])\n",
    "    test_size = int(test_ratio * len(rated_items_by_user))\n",
    "    test_indices = np.random.randint(0, len(rated_items_by_user), test_size)\n",
    "    \n",
    "    withheld_items = rated_items_by_user[test_indices]\n",
    "    original_user_profile = np.copy(dataMat[user])\n",
    "    dataMat[user, withheld_items] = 0 # So that the withheld test items is not used in the rating estimation below\n",
    "    error_u = 0.0\n",
    "    count_u = len(withheld_items)\n",
    "\n",
    "\t# Compute absolute error for user u over all test items\n",
    "    for item in withheld_items:\n",
    "        # Estimate rating on the withheld item\n",
    "        estimatedScore = estMethod(dataMat, user, simMeas, item)\n",
    "        error_u = error_u + (estimatedScore - original_user_profile[item])**2\n",
    "    \n",
    "    for item in withheld_items:\n",
    "        dataMat[user, item] = original_user_profile[item]\n",
    "    return error_u, count_u\n",
    "\n",
    "\n",
    "def test(dataMat, test_ratio, estMethod):\n",
    "    # iterate over all users and for each \n",
    "    # perform evaluation by calling the above cross_validate_user function on each user.\n",
    "    total_error=0.0\n",
    "    total_count=0\n",
    "    for user in range(dataMat.shape[0]):\n",
    "        if estMethod == 'standEst':\n",
    "            error_u, count_u = cross_validate_user(dataMat,user,test_ratio, standEst)\n",
    "            total_error += error_u\n",
    "            total_count += count_u\n",
    "        if estMethod == 'svdEst':\n",
    "            if user % 50 ==0:\n",
    "                error_u, count_u = cross_validate_user(dataMat,user,test_ratio,svdEst)\n",
    "                total_error += error_u\n",
    "                total_count += count_u\n",
    "    RMSE= np.sqrt(np.mean(total_error))\n",
    "    print('Mean Absoloute Error for ',estMethod,' : ', RMSE)\n",
    "    return RMSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "c29d0d6d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(59132, 140)"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mat(rating_df).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "412b62fe",
   "metadata": {},
   "source": [
    "By iterating all users and for each performs evaluation with 20% test ratio, the overall Mean Absolute Error (MAE) using standard item-based collaborative filtering based on the rating prediction function \"standEst\" is approximately 3.69. Meantime, the MAE of using the SVD-based version of the rating item-based CF as the prediction engine, is approximately 4.42. Notice that the computing time for using 'svdEst' function is much more efficient than using 'standEst' function/ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "8a93d95f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absoloute Error for  <function standEst at 0x7fbf42fea040>  :  0.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test(rating_df, 0.3, standEst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06470be9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_most_similar_jokes(dataMat, jokes, queryJoke, k, metric=pearsSim):\n",
    "    # Write this function to find the k most similar jokes \n",
    "    #(based on user ratings) to a queryJoke. \n",
    "    #The queryJoke is a joke id as given in the 'jokes.csv'.\n",
    "    jokeT= jester.T\n",
    "    SimList=pd.DataFrame({\"index\":[],\"distance\":[]})\n",
    "    for i in range(jokeT.shape[0]):\n",
    "        dist= pearsSim(jokeT[5],jokeT[i])\n",
    "        SimList= SimList.append({\"index\":i,'distance':dist},ignore_index=True)\n",
    "    \n",
    "    SimList= SimList.sort_values(by=['distance'],ascending=False)\n",
    "    SimList= SimList.reset_index(drop=True)    \n",
    "    outSim= SimList[1:int(k+1)]\n",
    "    print(\"The \", k, \" top similar jokes for Joke\",queryJoke, ':\\n',\n",
    "          jokes[1][queryJoke], \" are:\")\n",
    "    for joke in range(len(outSim)):\n",
    "        txt= outSim['index'][int(joke+1)]\n",
    "        print('\\nJoke',outSim['index'][int(joke+1)],':\\n',jokes[1][txt])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73affa31",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print_most_similar_jokes(jester, jokes, 4, 5, metric=pearsSim)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
